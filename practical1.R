#3
setwd("put/your/local/repo/location/here")
a <- scan("pg10.txt",what="character",skip=104) ## skip contents
n <- length(a)
a <- a[-((n-2886):n)] ## strip license
a <- a[-grep("[0123456789]:[0123456789]",a)] ## strip out verse numbers 

#4
split_punct<-function(a){
  ia<- grep("[[:punct:]]",a,fixed=FALSE) # which element of a has punctuation
  xia<- rep("",length(ia)+length(a)) # vector to store the elements without punctuation
  iis <- ia+1:length(ia) #where should punctuation go
  xs<-c()
  xs[iis]<- substr(a[ia],nchar(a[ia]),nchar(a[ia]))
  xs[-iis]<- gsub("[[:punct:]]","",a) #without punctuation
  return(xs)
}

#5
a<-split_punct(a)#separate the punctuation marks

#6(a)
a<-tolower(a)#replaced the capital letters in words with lower case letters
uniq<-unique(a)#find the vector of unique words in a

#6(b)
#find the vector of indices indicating which element 
#in the unique word vector
idx_vector<-match(a,uniq)

#6(c)
count<-tabulate(idx_vector)#count up how many time each unique word occurs in the text

#6(d)(e)
m<-500#decide the threshold number of occurrence
idx_m<-order(count,decreasing = TRUE)[1:m]#sort the index of occurrence
b<-c()#create a vector b
#b contains m(500) most commonly occurring words
for (i in idx_m) { b<-append(b,uniq[i])}

#7a
idx_common<-match(a,b)#first column

#7b
secondc<-append(idx_common[2:length(idx_common)],NA)#second column
thirdc<-append(secondc[2:length(secondc)],NA)#third column
mat<-cbind(idx_common,secondc,thirdc)#form a matrix

#7c
triplet_all<-rowSums(is.na(mat))
idx_triplet_com<-which(triplet_all==0)#identify the common word triplets
triplet_com<-mat[idx_triplet_com,]#drop the other word triplets those that contain an NA

#7d
T<-array(0,dim=c(m,m,m))#initialize matrix T with 500x500x500
#loop through the common word triplets adding a 1 to T[i,k,j] every time the jth common word follows the pair i,k
for (i in 1:nrow(triplet_com)) {
  T[triplet_com[i,1],triplet_com[i,2],triplet_com[i,3]]<-T[triplet_com[i,1],triplet_com[i,2],triplet_com[i,3]]+1
}


#7f
A<-array(0,dim=c(m,m))#initialize matrix A with 500x500
#loop through the common word triplets adding a 1 to A[i,k] every time the kth common word follows the word i
for (i in 1:nrow(triplet_com)) {
  A[triplet_com[i,1],triplet_com[i,2]]<-A[triplet_com[i,1],triplet_com[i,2]]+1
}

S<-rep(0,m)##initialize vector s with 1x500
#loop through the common word triplets adding a 1 to S[i] every time the ith common word occur
for (i in 1:nrow(triplet_com)) {
  S[triplet_com[i,1]]<-S[triplet_com[i,1]]+1
}

#8
#simulate is a function used to simulate 50-word sections from the model.It takes 5 arguments:
#b: a vector of the m most commonly words
#S: a vector that counts up the number of times that each bi occurs in the text
#A: a matrix that counts up each time bj follows bi in the text.
#T: a matrix that counts up the number of times bj follows sequential word pairs bi, bk for all words
#n: the number of words in a given word-section

simulate<-function(b,S,A,T,n){
  generate_string='' #this string will be the final result to generate
  sample_S<- sample(b,1,prob = S) # first randomly pick a word from b, based on the probabilities in S
  
  #then randomly pick a word from b again, based on the first word and the probabilities in A
  sample_A<-sample(b,1,prob=A[which(b==sample_S),])
  generate_string=paste(generate_string,sample_S,sample_A) # record the two words
  
  #since the initial pair of words have been picked, loop n-2 times to randomly pick a word from b,
  #each time based on the previous two generated words and the the probabilities in T, until all words 
  #have been generated
  for (i in 1:n-2) {
    #sample_T is the words generated based on sample_S, sample_A and the prob of T
    sample_T<-sample(b,1,prob=T[which(b==sample_S),which(b==sample_A),])
    
    generate_string=paste(generate_string,sample_T)
    
    #update sample_S and sample_A
    sample_S<-sample_A
    sample_A<-sample_T
  }
  cat(generate_string) #print out the corresponding text
}

simulate(b,S,A,T,50)#now run the simulate function to see what kind of text will be generated!

#9
#simulate 50 word sections of text where the word probabilities are simply taken from S
cat(sample(b,50,prob = S))

##Finally, we can conclude that there exists a huge difference between the results of using
#2nd order Markov model and randomly picking words. The text generated by the former idea is
#more structured and reasonable than that generated by the latter.